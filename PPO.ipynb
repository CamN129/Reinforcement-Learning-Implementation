{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c8209ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "from torch.distributions import MultivariateNormal\n",
    "import numpy as np\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d26b11a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class policynet(nn.Module):\n",
    "    def __init__(self, s, h, h2, a): # s-state space, hidden layer size, second hidden layer size, action space\n",
    "        super(policynet, self).__init__()\n",
    "        self.hl = nn.Linear(s,h)\n",
    "        self.hl2 = nn.Linear(h,h2)\n",
    "        self.out = nn.Linear(h2,a)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.hl(x))\n",
    "        x = F.relu(self.hl2(x))\n",
    "        x = self.out(x)\n",
    "        \n",
    "        return x\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a2f2c815",
   "metadata": {},
   "outputs": [],
   "source": [
    "## some hyper parameters\n",
    "env_name='Pendulum-v0'\n",
    "hidden_sizes=[32]\n",
    "lr=5e-3\n",
    "total_time=1000000\n",
    "time_per_batch = 3000\n",
    "max_steps_episode = 1500\n",
    "gamma = .95 # discount factor\n",
    "updates_per_iteration = 5\n",
    "clip = 0.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c61a929",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 3000 \t reward: -1218.923 \t ep_len: 200.000\n",
      "time: 6000 \t reward: -1341.225 \t ep_len: 200.000\n",
      "time: 9000 \t reward: -1289.657 \t ep_len: 200.000\n",
      "time: 12000 \t reward: -1254.246 \t ep_len: 200.000\n",
      "time: 15000 \t reward: -1219.515 \t ep_len: 200.000\n",
      "time: 18000 \t reward: -1184.793 \t ep_len: 200.000\n",
      "time: 21000 \t reward: -1183.111 \t ep_len: 200.000\n",
      "time: 24000 \t reward: -1187.248 \t ep_len: 200.000\n",
      "time: 27000 \t reward: -1244.862 \t ep_len: 200.000\n",
      "time: 30000 \t reward: -1213.684 \t ep_len: 200.000\n",
      "time: 33000 \t reward: -1204.177 \t ep_len: 200.000\n",
      "time: 36000 \t reward: -1215.105 \t ep_len: 200.000\n",
      "time: 39000 \t reward: -1141.333 \t ep_len: 200.000\n",
      "time: 42000 \t reward: -1218.544 \t ep_len: 200.000\n",
      "time: 45000 \t reward: -1179.629 \t ep_len: 200.000\n",
      "time: 48000 \t reward: -1000.417 \t ep_len: 200.000\n",
      "time: 51000 \t reward: -1036.261 \t ep_len: 200.000\n",
      "time: 54000 \t reward: -1036.914 \t ep_len: 200.000\n",
      "time: 57000 \t reward: -1145.020 \t ep_len: 200.000\n",
      "time: 60000 \t reward: -1077.608 \t ep_len: 200.000\n",
      "time: 63000 \t reward: -1049.102 \t ep_len: 200.000\n",
      "time: 66000 \t reward: -1030.812 \t ep_len: 200.000\n",
      "time: 69000 \t reward: -971.854 \t ep_len: 200.000\n",
      "time: 72000 \t reward: -952.675 \t ep_len: 200.000\n",
      "time: 75000 \t reward: -973.696 \t ep_len: 200.000\n",
      "time: 78000 \t reward: -943.947 \t ep_len: 200.000\n",
      "time: 81000 \t reward: -877.309 \t ep_len: 200.000\n",
      "time: 84000 \t reward: -873.566 \t ep_len: 200.000\n",
      "time: 87000 \t reward: -866.964 \t ep_len: 200.000\n",
      "time: 90000 \t reward: -793.260 \t ep_len: 200.000\n",
      "time: 93000 \t reward: -860.833 \t ep_len: 200.000\n",
      "time: 96000 \t reward: -871.173 \t ep_len: 200.000\n",
      "time: 99000 \t reward: -891.753 \t ep_len: 200.000\n",
      "time: 102000 \t reward: -780.230 \t ep_len: 200.000\n",
      "time: 105000 \t reward: -753.987 \t ep_len: 200.000\n",
      "time: 108000 \t reward: -805.016 \t ep_len: 200.000\n",
      "time: 111000 \t reward: -823.428 \t ep_len: 200.000\n",
      "time: 114000 \t reward: -755.362 \t ep_len: 200.000\n",
      "time: 117000 \t reward: -745.019 \t ep_len: 200.000\n",
      "time: 120000 \t reward: -647.432 \t ep_len: 200.000\n",
      "time: 123000 \t reward: -642.468 \t ep_len: 200.000\n",
      "time: 126000 \t reward: -439.644 \t ep_len: 200.000\n",
      "time: 129000 \t reward: -501.866 \t ep_len: 200.000\n",
      "time: 132000 \t reward: -322.529 \t ep_len: 200.000\n",
      "time: 135000 \t reward: -409.073 \t ep_len: 200.000\n",
      "time: 138000 \t reward: -374.158 \t ep_len: 200.000\n",
      "time: 141000 \t reward: -228.820 \t ep_len: 200.000\n",
      "time: 144000 \t reward: -244.601 \t ep_len: 200.000\n",
      "time: 147000 \t reward: -263.229 \t ep_len: 200.000\n",
      "time: 150000 \t reward: -142.729 \t ep_len: 200.000\n",
      "time: 153000 \t reward: -162.341 \t ep_len: 200.000\n",
      "time: 156000 \t reward: -240.826 \t ep_len: 200.000\n",
      "time: 159000 \t reward: -242.707 \t ep_len: 200.000\n",
      "time: 162000 \t reward: -197.976 \t ep_len: 200.000\n",
      "time: 165000 \t reward: -168.627 \t ep_len: 200.000\n",
      "time: 168000 \t reward: -199.146 \t ep_len: 200.000\n",
      "time: 171000 \t reward: -277.765 \t ep_len: 200.000\n",
      "time: 174000 \t reward: -323.780 \t ep_len: 200.000\n",
      "time: 177000 \t reward: -195.736 \t ep_len: 200.000\n",
      "time: 180000 \t reward: -242.130 \t ep_len: 200.000\n",
      "time: 183000 \t reward: -255.296 \t ep_len: 200.000\n",
      "time: 186000 \t reward: -234.592 \t ep_len: 200.000\n",
      "time: 189000 \t reward: -213.190 \t ep_len: 200.000\n",
      "time: 192000 \t reward: -197.352 \t ep_len: 200.000\n",
      "time: 195000 \t reward: -210.660 \t ep_len: 200.000\n",
      "time: 198000 \t reward: -222.876 \t ep_len: 200.000\n",
      "time: 201000 \t reward: -212.828 \t ep_len: 200.000\n",
      "time: 204000 \t reward: -167.261 \t ep_len: 200.000\n",
      "time: 207000 \t reward: -305.800 \t ep_len: 200.000\n",
      "time: 210000 \t reward: -205.640 \t ep_len: 200.000\n",
      "time: 213000 \t reward: -224.488 \t ep_len: 200.000\n",
      "time: 216000 \t reward: -221.996 \t ep_len: 200.000\n",
      "time: 219000 \t reward: -170.208 \t ep_len: 200.000\n",
      "time: 222000 \t reward: -209.281 \t ep_len: 200.000\n",
      "time: 225000 \t reward: -160.231 \t ep_len: 200.000\n",
      "time: 228000 \t reward: -168.116 \t ep_len: 200.000\n",
      "time: 231000 \t reward: -167.082 \t ep_len: 200.000\n",
      "time: 234000 \t reward: -205.446 \t ep_len: 200.000\n",
      "time: 237000 \t reward: -172.005 \t ep_len: 200.000\n",
      "time: 240000 \t reward: -245.471 \t ep_len: 200.000\n",
      "time: 243000 \t reward: -135.159 \t ep_len: 200.000\n",
      "time: 246000 \t reward: -201.781 \t ep_len: 200.000\n",
      "time: 249000 \t reward: -154.160 \t ep_len: 200.000\n",
      "time: 252000 \t reward: -178.539 \t ep_len: 200.000\n",
      "time: 255000 \t reward: -151.322 \t ep_len: 200.000\n",
      "time: 258000 \t reward: -187.230 \t ep_len: 200.000\n",
      "time: 261000 \t reward: -150.136 \t ep_len: 200.000\n",
      "time: 264000 \t reward: -160.913 \t ep_len: 200.000\n",
      "time: 267000 \t reward: -187.743 \t ep_len: 200.000\n",
      "time: 270000 \t reward: -136.034 \t ep_len: 200.000\n",
      "time: 273000 \t reward: -163.983 \t ep_len: 200.000\n",
      "time: 276000 \t reward: -215.411 \t ep_len: 200.000\n",
      "time: 279000 \t reward: -211.486 \t ep_len: 200.000\n",
      "time: 282000 \t reward: -144.442 \t ep_len: 200.000\n",
      "time: 285000 \t reward: -165.813 \t ep_len: 200.000\n",
      "time: 288000 \t reward: -232.762 \t ep_len: 200.000\n",
      "time: 291000 \t reward: -175.512 \t ep_len: 200.000\n",
      "time: 294000 \t reward: -194.470 \t ep_len: 200.000\n",
      "time: 297000 \t reward: -157.107 \t ep_len: 200.000\n",
      "time: 300000 \t reward: -124.823 \t ep_len: 200.000\n",
      "time: 303000 \t reward: -182.416 \t ep_len: 200.000\n",
      "time: 306000 \t reward: -163.608 \t ep_len: 200.000\n",
      "time: 309000 \t reward: -135.663 \t ep_len: 200.000\n",
      "time: 312000 \t reward: -149.920 \t ep_len: 200.000\n",
      "time: 315000 \t reward: -166.111 \t ep_len: 200.000\n",
      "time: 318000 \t reward: -153.647 \t ep_len: 200.000\n",
      "time: 321000 \t reward: -230.210 \t ep_len: 200.000\n",
      "time: 324000 \t reward: -109.704 \t ep_len: 200.000\n",
      "time: 327000 \t reward: -147.523 \t ep_len: 200.000\n",
      "time: 330000 \t reward: -199.168 \t ep_len: 200.000\n",
      "time: 333000 \t reward: -210.786 \t ep_len: 200.000\n",
      "time: 336000 \t reward: -158.822 \t ep_len: 200.000\n",
      "time: 339000 \t reward: -218.104 \t ep_len: 200.000\n",
      "time: 342000 \t reward: -146.904 \t ep_len: 200.000\n",
      "time: 345000 \t reward: -148.854 \t ep_len: 200.000\n",
      "time: 348000 \t reward: -156.351 \t ep_len: 200.000\n",
      "time: 351000 \t reward: -161.787 \t ep_len: 200.000\n",
      "time: 354000 \t reward: -190.409 \t ep_len: 200.000\n",
      "time: 357000 \t reward: -158.402 \t ep_len: 200.000\n",
      "time: 360000 \t reward: -146.553 \t ep_len: 200.000\n",
      "time: 363000 \t reward: -151.062 \t ep_len: 200.000\n",
      "time: 366000 \t reward: -151.152 \t ep_len: 200.000\n",
      "time: 369000 \t reward: -128.179 \t ep_len: 200.000\n",
      "time: 372000 \t reward: -143.850 \t ep_len: 200.000\n",
      "time: 375000 \t reward: -176.168 \t ep_len: 200.000\n",
      "time: 378000 \t reward: -161.635 \t ep_len: 200.000\n",
      "time: 381000 \t reward: -193.008 \t ep_len: 200.000\n",
      "time: 384000 \t reward: -160.936 \t ep_len: 200.000\n",
      "time: 387000 \t reward: -149.496 \t ep_len: 200.000\n",
      "time: 390000 \t reward: -136.006 \t ep_len: 200.000\n",
      "time: 393000 \t reward: -180.404 \t ep_len: 200.000\n",
      "time: 396000 \t reward: -148.410 \t ep_len: 200.000\n",
      "time: 399000 \t reward: -135.948 \t ep_len: 200.000\n",
      "time: 402000 \t reward: -136.970 \t ep_len: 200.000\n",
      "time: 405000 \t reward: -171.479 \t ep_len: 200.000\n",
      "time: 408000 \t reward: -175.761 \t ep_len: 200.000\n",
      "time: 411000 \t reward: -151.742 \t ep_len: 200.000\n",
      "time: 414000 \t reward: -144.828 \t ep_len: 200.000\n",
      "time: 417000 \t reward: -161.437 \t ep_len: 200.000\n",
      "time: 420000 \t reward: -151.481 \t ep_len: 200.000\n",
      "time: 423000 \t reward: -202.215 \t ep_len: 200.000\n",
      "time: 426000 \t reward: -134.723 \t ep_len: 200.000\n",
      "time: 429000 \t reward: -185.653 \t ep_len: 200.000\n",
      "time: 432000 \t reward: -162.325 \t ep_len: 200.000\n",
      "time: 435000 \t reward: -177.073 \t ep_len: 200.000\n",
      "time: 438000 \t reward: -166.877 \t ep_len: 200.000\n",
      "time: 441000 \t reward: -141.053 \t ep_len: 200.000\n",
      "time: 444000 \t reward: -153.550 \t ep_len: 200.000\n",
      "time: 447000 \t reward: -189.660 \t ep_len: 200.000\n",
      "time: 450000 \t reward: -190.274 \t ep_len: 200.000\n",
      "time: 453000 \t reward: -172.341 \t ep_len: 200.000\n",
      "time: 456000 \t reward: -184.558 \t ep_len: 200.000\n",
      "time: 459000 \t reward: -231.888 \t ep_len: 200.000\n",
      "time: 462000 \t reward: -168.748 \t ep_len: 200.000\n",
      "time: 465000 \t reward: -205.141 \t ep_len: 200.000\n",
      "time: 468000 \t reward: -148.375 \t ep_len: 200.000\n",
      "time: 471000 \t reward: -143.629 \t ep_len: 200.000\n",
      "time: 474000 \t reward: -155.140 \t ep_len: 200.000\n",
      "time: 477000 \t reward: -145.002 \t ep_len: 200.000\n",
      "time: 480000 \t reward: -172.238 \t ep_len: 200.000\n",
      "time: 483000 \t reward: -159.920 \t ep_len: 200.000\n",
      "time: 486000 \t reward: -145.043 \t ep_len: 200.000\n",
      "time: 489000 \t reward: -153.564 \t ep_len: 200.000\n",
      "time: 492000 \t reward: -136.696 \t ep_len: 200.000\n",
      "time: 495000 \t reward: -139.052 \t ep_len: 200.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 498000 \t reward: -224.439 \t ep_len: 200.000\n",
      "time: 501000 \t reward: -180.687 \t ep_len: 200.000\n",
      "time: 504000 \t reward: -182.592 \t ep_len: 200.000\n",
      "time: 507000 \t reward: -171.317 \t ep_len: 200.000\n",
      "time: 510000 \t reward: -141.821 \t ep_len: 200.000\n",
      "time: 513000 \t reward: -167.635 \t ep_len: 200.000\n",
      "time: 516000 \t reward: -182.513 \t ep_len: 200.000\n",
      "time: 519000 \t reward: -170.660 \t ep_len: 200.000\n",
      "time: 522000 \t reward: -110.802 \t ep_len: 200.000\n",
      "time: 525000 \t reward: -142.170 \t ep_len: 200.000\n",
      "time: 528000 \t reward: -153.640 \t ep_len: 200.000\n",
      "time: 531000 \t reward: -217.211 \t ep_len: 200.000\n",
      "time: 534000 \t reward: -158.668 \t ep_len: 200.000\n",
      "time: 537000 \t reward: -136.337 \t ep_len: 200.000\n",
      "time: 540000 \t reward: -93.810 \t ep_len: 200.000\n",
      "time: 543000 \t reward: -167.153 \t ep_len: 200.000\n",
      "time: 546000 \t reward: -144.865 \t ep_len: 200.000\n",
      "time: 549000 \t reward: -162.022 \t ep_len: 200.000\n",
      "time: 552000 \t reward: -178.888 \t ep_len: 200.000\n",
      "time: 555000 \t reward: -146.996 \t ep_len: 200.000\n",
      "time: 558000 \t reward: -133.977 \t ep_len: 200.000\n",
      "time: 561000 \t reward: -137.031 \t ep_len: 200.000\n",
      "time: 564000 \t reward: -134.397 \t ep_len: 200.000\n",
      "time: 567000 \t reward: -162.884 \t ep_len: 200.000\n",
      "time: 570000 \t reward: -132.994 \t ep_len: 200.000\n",
      "time: 573000 \t reward: -160.421 \t ep_len: 200.000\n",
      "time: 576000 \t reward: -152.470 \t ep_len: 200.000\n",
      "time: 579000 \t reward: -130.319 \t ep_len: 200.000\n",
      "time: 582000 \t reward: -161.743 \t ep_len: 200.000\n",
      "time: 585000 \t reward: -137.809 \t ep_len: 200.000\n",
      "time: 588000 \t reward: -149.151 \t ep_len: 200.000\n",
      "time: 591000 \t reward: -170.967 \t ep_len: 200.000\n",
      "time: 594000 \t reward: -178.205 \t ep_len: 200.000\n",
      "time: 597000 \t reward: -172.925 \t ep_len: 200.000\n",
      "time: 600000 \t reward: -141.885 \t ep_len: 200.000\n",
      "time: 603000 \t reward: -165.266 \t ep_len: 200.000\n",
      "time: 606000 \t reward: -148.676 \t ep_len: 200.000\n",
      "time: 609000 \t reward: -151.883 \t ep_len: 200.000\n",
      "time: 612000 \t reward: -177.518 \t ep_len: 200.000\n",
      "time: 615000 \t reward: -200.914 \t ep_len: 200.000\n",
      "time: 618000 \t reward: -164.853 \t ep_len: 200.000\n",
      "time: 621000 \t reward: -156.528 \t ep_len: 200.000\n",
      "time: 624000 \t reward: -144.770 \t ep_len: 200.000\n",
      "time: 627000 \t reward: -188.907 \t ep_len: 200.000\n",
      "time: 630000 \t reward: -152.385 \t ep_len: 200.000\n",
      "time: 633000 \t reward: -170.943 \t ep_len: 200.000\n",
      "time: 636000 \t reward: -147.800 \t ep_len: 200.000\n",
      "time: 639000 \t reward: -101.460 \t ep_len: 200.000\n",
      "time: 642000 \t reward: -176.734 \t ep_len: 200.000\n",
      "time: 645000 \t reward: -115.412 \t ep_len: 200.000\n",
      "time: 648000 \t reward: -125.313 \t ep_len: 200.000\n",
      "time: 651000 \t reward: -177.745 \t ep_len: 200.000\n",
      "time: 654000 \t reward: -180.567 \t ep_len: 200.000\n",
      "time: 657000 \t reward: -167.579 \t ep_len: 200.000\n",
      "time: 660000 \t reward: -217.261 \t ep_len: 200.000\n",
      "time: 663000 \t reward: -122.723 \t ep_len: 200.000\n",
      "time: 666000 \t reward: -142.546 \t ep_len: 200.000\n",
      "time: 669000 \t reward: -151.835 \t ep_len: 200.000\n",
      "time: 672000 \t reward: -171.475 \t ep_len: 200.000\n",
      "time: 675000 \t reward: -162.595 \t ep_len: 200.000\n",
      "time: 678000 \t reward: -126.568 \t ep_len: 200.000\n",
      "time: 681000 \t reward: -161.082 \t ep_len: 200.000\n",
      "time: 684000 \t reward: -159.757 \t ep_len: 200.000\n",
      "time: 687000 \t reward: -141.697 \t ep_len: 200.000\n",
      "time: 690000 \t reward: -156.737 \t ep_len: 200.000\n",
      "time: 693000 \t reward: -125.941 \t ep_len: 200.000\n",
      "time: 696000 \t reward: -118.074 \t ep_len: 200.000\n",
      "time: 699000 \t reward: -151.393 \t ep_len: 200.000\n",
      "time: 702000 \t reward: -165.885 \t ep_len: 200.000\n",
      "time: 705000 \t reward: -196.258 \t ep_len: 200.000\n",
      "time: 708000 \t reward: -125.508 \t ep_len: 200.000\n",
      "time: 711000 \t reward: -173.991 \t ep_len: 200.000\n",
      "time: 714000 \t reward: -182.776 \t ep_len: 200.000\n",
      "time: 717000 \t reward: -132.546 \t ep_len: 200.000\n",
      "time: 720000 \t reward: -130.571 \t ep_len: 200.000\n",
      "time: 723000 \t reward: -159.379 \t ep_len: 200.000\n",
      "time: 726000 \t reward: -121.692 \t ep_len: 200.000\n",
      "time: 729000 \t reward: -139.703 \t ep_len: 200.000\n",
      "time: 732000 \t reward: -132.537 \t ep_len: 200.000\n",
      "time: 735000 \t reward: -183.874 \t ep_len: 200.000\n",
      "time: 738000 \t reward: -196.410 \t ep_len: 200.000\n",
      "time: 741000 \t reward: -125.040 \t ep_len: 200.000\n",
      "time: 744000 \t reward: -142.332 \t ep_len: 200.000\n",
      "time: 747000 \t reward: -174.364 \t ep_len: 200.000\n",
      "time: 750000 \t reward: -150.445 \t ep_len: 200.000\n",
      "time: 753000 \t reward: -140.668 \t ep_len: 200.000\n",
      "time: 756000 \t reward: -155.712 \t ep_len: 200.000\n",
      "time: 759000 \t reward: -170.221 \t ep_len: 200.000\n",
      "time: 762000 \t reward: -161.114 \t ep_len: 200.000\n",
      "time: 765000 \t reward: -141.376 \t ep_len: 200.000\n",
      "time: 768000 \t reward: -123.373 \t ep_len: 200.000\n",
      "time: 771000 \t reward: -137.144 \t ep_len: 200.000\n",
      "time: 774000 \t reward: -172.515 \t ep_len: 200.000\n",
      "time: 777000 \t reward: -176.683 \t ep_len: 200.000\n",
      "time: 780000 \t reward: -202.029 \t ep_len: 200.000\n",
      "time: 783000 \t reward: -165.545 \t ep_len: 200.000\n",
      "time: 786000 \t reward: -138.717 \t ep_len: 200.000\n",
      "time: 789000 \t reward: -149.974 \t ep_len: 200.000\n",
      "time: 792000 \t reward: -193.637 \t ep_len: 200.000\n",
      "time: 795000 \t reward: -149.131 \t ep_len: 200.000\n",
      "time: 798000 \t reward: -151.725 \t ep_len: 200.000\n",
      "time: 801000 \t reward: -156.701 \t ep_len: 200.000\n",
      "time: 804000 \t reward: -185.674 \t ep_len: 200.000\n",
      "time: 807000 \t reward: -132.371 \t ep_len: 200.000\n",
      "time: 810000 \t reward: -173.151 \t ep_len: 200.000\n",
      "time: 813000 \t reward: -198.010 \t ep_len: 200.000\n",
      "time: 816000 \t reward: -109.039 \t ep_len: 200.000\n",
      "time: 819000 \t reward: -215.161 \t ep_len: 200.000\n",
      "time: 822000 \t reward: -159.010 \t ep_len: 200.000\n",
      "time: 825000 \t reward: -168.726 \t ep_len: 200.000\n",
      "time: 828000 \t reward: -124.008 \t ep_len: 200.000\n",
      "time: 831000 \t reward: -156.634 \t ep_len: 200.000\n",
      "time: 834000 \t reward: -93.824 \t ep_len: 200.000\n",
      "time: 837000 \t reward: -169.842 \t ep_len: 200.000\n",
      "time: 840000 \t reward: -167.432 \t ep_len: 200.000\n",
      "time: 843000 \t reward: -147.873 \t ep_len: 200.000\n",
      "time: 846000 \t reward: -172.285 \t ep_len: 200.000\n",
      "time: 849000 \t reward: -109.358 \t ep_len: 200.000\n",
      "time: 852000 \t reward: -172.581 \t ep_len: 200.000\n",
      "time: 855000 \t reward: -133.864 \t ep_len: 200.000\n",
      "time: 858000 \t reward: -122.985 \t ep_len: 200.000\n",
      "time: 861000 \t reward: -162.349 \t ep_len: 200.000\n",
      "time: 864000 \t reward: -168.218 \t ep_len: 200.000\n",
      "time: 867000 \t reward: -217.968 \t ep_len: 200.000\n",
      "time: 870000 \t reward: -219.374 \t ep_len: 200.000\n",
      "time: 873000 \t reward: -142.973 \t ep_len: 200.000\n",
      "time: 876000 \t reward: -173.885 \t ep_len: 200.000\n",
      "time: 879000 \t reward: -213.494 \t ep_len: 200.000\n",
      "time: 882000 \t reward: -171.077 \t ep_len: 200.000\n",
      "time: 885000 \t reward: -267.132 \t ep_len: 200.000\n",
      "time: 888000 \t reward: -201.498 \t ep_len: 200.000\n",
      "time: 891000 \t reward: -182.619 \t ep_len: 200.000\n",
      "time: 894000 \t reward: -168.580 \t ep_len: 200.000\n",
      "time: 897000 \t reward: -206.993 \t ep_len: 200.000\n",
      "time: 900000 \t reward: -163.889 \t ep_len: 200.000\n",
      "time: 903000 \t reward: -183.761 \t ep_len: 200.000\n",
      "time: 906000 \t reward: -169.967 \t ep_len: 200.000\n",
      "time: 909000 \t reward: -148.711 \t ep_len: 200.000\n",
      "time: 912000 \t reward: -162.231 \t ep_len: 200.000\n",
      "time: 915000 \t reward: -173.051 \t ep_len: 200.000\n",
      "time: 918000 \t reward: -154.913 \t ep_len: 200.000\n",
      "time: 921000 \t reward: -168.917 \t ep_len: 200.000\n",
      "time: 924000 \t reward: -153.621 \t ep_len: 200.000\n",
      "time: 927000 \t reward: -197.776 \t ep_len: 200.000\n",
      "time: 930000 \t reward: -112.069 \t ep_len: 200.000\n",
      "time: 933000 \t reward: -136.490 \t ep_len: 200.000\n",
      "time: 936000 \t reward: -194.466 \t ep_len: 200.000\n",
      "time: 939000 \t reward: -144.376 \t ep_len: 200.000\n",
      "time: 942000 \t reward: -134.027 \t ep_len: 200.000\n",
      "time: 945000 \t reward: -166.160 \t ep_len: 200.000\n"
     ]
    }
   ],
   "source": [
    "# make environment\n",
    "env = gym.make(env_name)\n",
    "\n",
    "# get dimensions for policy network\n",
    "\n",
    "#observation dimensions\n",
    "obs_dim = env.observation_space.shape[0]\n",
    "\n",
    "# action dimensions\n",
    "n_acts = env.action_space.shape[0]\n",
    "\n",
    "# make actor and critic policy networks\n",
    "\n",
    "# this suggests actions for us\n",
    "actor = policynet(obs_dim, 50,50, n_acts)\n",
    "\n",
    "# this estimates value for us\n",
    "critic = policynet(obs_dim, 50,50, 1)\n",
    "\n",
    "# actor adam optimizer\n",
    "\n",
    "a_optimizer = torch.optim.Adam(actor.parameters(), lr=lr)\n",
    "\n",
    "# critic optimizer\n",
    "\n",
    "c_optimizer = torch.optim.Adam(critic.parameters(), lr=lr)\n",
    "\n",
    "# covariance matrix for normal distribution\n",
    "cov_vec = torch.full(size=(n_acts,), fill_value=0.5)\n",
    "cov_mat = torch.diag(cov_vec)\n",
    "\n",
    "\n",
    "# make action selection function (outputs int actions, sampled from policy)\n",
    "def get_action(obs):\n",
    "    \n",
    "    # mean action\n",
    "    mean = actor(obs)\n",
    "    \n",
    "    # create distribution\n",
    "    dist = MultivariateNormal(mean, cov_mat)\n",
    "    \n",
    "    #get action\n",
    "    act = dist.sample()\n",
    "    \n",
    "    # get log prob\n",
    "    log_prob = dist.log_prob(act)\n",
    "    \n",
    "    # return action and log prob, don't add to existing computational graph\n",
    "    return act.detach().numpy(), log_prob.detach()\n",
    "    \n",
    "\n",
    "# compute rewards to go, rewards gained after taking an action\n",
    "def compute_rtg(batch_rew):\n",
    "    batch_rtg = []\n",
    "    \n",
    "    for ep_rews in reversed(batch_rew):\n",
    "        \n",
    "        # discounted reward\n",
    "        discounted_reward = 0\n",
    "        \n",
    "        for rew in reversed(ep_rews):\n",
    "            # getting rewards by going backwards is very simple last action has one reward, second to\n",
    "            # last has two subsequent rewards which is gamma times last action reward, etc.\n",
    "            discounted_reward = rew + discounted_reward * gamma\n",
    "            \n",
    "            # can't append since we are going backwards\n",
    "            batch_rtg.insert(0, discounted_reward)\n",
    "    \n",
    "    batch_rtg = torch.tensor(batch_rtg, dtype=torch.float)\n",
    "    return batch_rtg\n",
    "\n",
    "\n",
    "def train():\n",
    "\n",
    "    batch_obs = []          # for observations\n",
    "    batch_acts = []         # for actions\n",
    "    batch_rtg = []      # for rewards to go\n",
    "    batch_rew = []         # for measuring rewards\n",
    "    batch_lens = []         # for measuring episode lengths\n",
    "    batch_log_probs = []    # log probs\n",
    "\n",
    "    t = 0\n",
    "    ep_rews = []            \n",
    "    \n",
    "    while t < time_per_batch:\n",
    "        # reset episode-specific variables\n",
    "        obs = env.reset()      \n",
    "        done = False            # tracks if episode has ended\n",
    "        ep_rews = []            # episode rewards\n",
    "\n",
    "        # collect experience by acting in the environment with current policy\n",
    "        for ep_t in range(max_steps_episode):\n",
    "            # save obs\n",
    "            \n",
    "            t+=1\n",
    "            batch_obs.append(obs.copy())\n",
    "\n",
    "            # act in the environment\n",
    "            act, log_prob = get_action(torch.as_tensor(obs, dtype=torch.float32))\n",
    "            obs, rew, done, _ = env.step(act)\n",
    "\n",
    "            # save action,reward, log probs\n",
    "            batch_acts.append(act)\n",
    "            ep_rews.append(rew)\n",
    "            batch_log_probs.append(log_prob)\n",
    "\n",
    "            # check if episode is done, then record some info\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        ## track episode reward and length\n",
    "        #ep_len = len(ep_rews)\n",
    "        batch_rew.append(ep_rews)\n",
    "        batch_lens.append(ep_t+1)\n",
    "        \n",
    "    batch_obs = torch.tensor(np.array(batch_obs), dtype=torch.float)\n",
    "    batch_acts = torch.tensor(np.array(batch_acts), dtype=torch.float)\n",
    "    batch_log_probs = torch.tensor(batch_log_probs, dtype=torch.float)\n",
    "    batch_rtg = compute_rtg(batch_rew)    \n",
    "    \n",
    "    return batch_obs, batch_acts, batch_log_probs, batch_rtg, batch_lens, batch_rew\n",
    "\n",
    "# time so far\n",
    "t_yet = 0\n",
    "\n",
    "# iterations so far\n",
    "i_yet = 0\n",
    "\n",
    "while t_yet < total_time:\n",
    "    batch_obs, batch_acts, batch_log_probs, batch_rtg, batch_lens, batch_rew = train()\n",
    "    \n",
    "    t_yet += np.sum(batch_lens)\n",
    "    i_yet +=1\n",
    "    \n",
    "    # critic network gives value of actions\n",
    "    Val = critic(batch_obs).squeeze()\n",
    "    \n",
    "    # get log probs of actions according to actor network\n",
    "    #mean = actor(batch_obs)\n",
    "    #dist = MultivariateNormal(mean,cov_mat)\n",
    "    #log_prob = dist.log_probs(batch_acts)\n",
    "    \n",
    "    # get advantage values\n",
    "    A_k = batch_rtg - Val.detach()\n",
    "    \n",
    "    # normalize advantaage values for faster convergence\n",
    "    A_k = (A_k - A_k.mean()) / (A_k.std() + 1e-10)\n",
    "    \n",
    "    for _ in range(updates_per_iteration):\n",
    "        Val = critic(batch_obs).squeeze()\n",
    "    \n",
    "        # get log probs of actions according to actor network\n",
    "        mean = actor(batch_obs)\n",
    "        dist = MultivariateNormal(mean,cov_mat)\n",
    "        log_prob = dist.log_prob(batch_acts)\n",
    "        \n",
    "        # subtracting logs is like dividing, then exponentiate to get out of log space\n",
    "        ratios = torch.exp(log_prob - batch_log_probs)\n",
    "        \n",
    "        # calculate potential losses, one being the ppo clip.\n",
    "        # we take the minimum of the two as our loss\n",
    "        # because we want to limit change in action probability to clip\n",
    "        surr1 = ratios * A_k\n",
    "        surr2 = torch.clamp(ratios, 1 - clip, 1 + clip) * A_k\n",
    "        \n",
    "        # get losses, negative because we maximize actor \"loss\"\n",
    "        actor_loss = (-torch.min(surr1, surr2)).mean()\n",
    "        critic_loss = nn.MSELoss()(Val, batch_rtg)\n",
    "        \n",
    "        #backprop actor\n",
    "        a_optimizer.zero_grad()\n",
    "        actor_loss.backward(retain_graph=True)\n",
    "        a_optimizer.step()\n",
    "\n",
    "        # backprop critic\n",
    "        c_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        c_optimizer.step()\n",
    "\n",
    "    avg_ep_rews = np.mean([np.sum(ep_rews) for ep_rews in batch_rew])\n",
    "    \n",
    "    print('time: %3d \\t reward: %.3f \\t ep_len: %.3f'%\n",
    "            (t_yet, avg_ep_rews, np.mean(batch_lens)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b4c1a30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 200 timesteps\n"
     ]
    }
   ],
   "source": [
    "# test our model\n",
    "\n",
    "# use this if you want to save video output, otherwise comment out\n",
    "env = gym.wrappers.Monitor(gym.make('Pendulum-v0'), './', force = True)\n",
    "\n",
    "for i_episode in range(1):\n",
    "    \n",
    "    observation = env.reset()\n",
    "    \n",
    "    for t in range(200):\n",
    "        env.render()\n",
    "        #action = get_action(torch.as_tensor(observation, dtype=torch.float32))\n",
    "        action,_ = get_action(torch.as_tensor(observation, dtype=torch.float32))\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            print(\"Episode finished after {} timesteps\".format(t+1))\n",
    "            break\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
